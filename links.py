import re
import csv
import json
import time
import shutil
import sys
import tkinter as tk
from tkinter import ttk
from playwright.sync_api import sync_playwright

# ğŸ”¹ **Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø§ÙØ°Ø© `tkinter` Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬**
root = tk.Tk()
root.title("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙ†ÙÙŠØ°...")
root.geometry("300x150")
root.resizable(False, False)

# Ø¥Ø¶Ø§ÙØ© Ù†Øµ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Ø§ÙØ°Ø©
status_label = ttk.Label(root, text="ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ©...", font=("Arial", 12))
status_label.pack(pady=30)

# ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø§ÙØ°Ø©
root.update()

# ØªØ¹Ø±ÙŠÙ Ù†Ù…Ø· Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Regex
url_pattern = re.compile(r'(https?://)?(www\.)?(t\.me/[a-zA-Z0-9_]+|wa\.me/[0-9]+)')

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© (Ù†Ø¨Ø­Ø« ÙÙ‚Ø· Ø¹Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·)
target_domains = ["t.me", "wa.me"]

# Ø§Ø³Ù… Ù…Ù„Ù CSV Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠÙ‡
output_file = "result.csv"

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù CSV Ù…Ø¹ Ø±Ø£Ø³ Ø§Ù„Ø¬Ø¯ÙˆÙ„
with open(output_file, mode="w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["Username", "Links"])

with sync_playwright() as p:
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø± Chromium
    chromium_path = r"chromium\chrome.exe"

    if chromium_path:
        browser = p.chromium.launch(executable_path=chromium_path, headless=True)
    else:
        status_label.config(text="âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Chromium!")
        root.update()
        root.after(3000, root.destroy)
        sys.exit(1)

    context = browser.new_context()

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù…Ù† Ø§Ù„Ù…Ù„Ù
    with open("cookies.json", "r", encoding="utf-8") as f:
        cookies = json.load(f)
        context.add_cookies(cookies)

    page = context.new_page()
    page.goto("https://x.com/home")
    time.sleep(2)

    with open("users.txt", 'r', encoding='utf-8') as file:
        for username in file:
            username = username.strip()
            if username:
                user_url = f"{username}"
                print(f"ğŸ”¹ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ ØµÙØ­Ø©: {user_url}")
                page.goto(user_url)
                time.sleep(2)

                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø§ÙŠÙˆ
                    bio_element = page.query_selector('div[data-testid="UserDescription"]')
                    bio = bio_element.inner_text().strip() if bio_element else ""

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ø¨Ø§ÙŠÙˆ ÙˆØªØµÙÙŠØ© `target_domains`
                    bio_links_text = []
                    found_bio_links = url_pattern.findall(bio)
                    for match in found_bio_links:
                        protocol = "https://" if not match[0] else match[0]
                        full_url = protocol + match[2]

                        if any(domain in full_url for domain in target_domains):
                            bio_links_text.append(full_url)

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† UserUrl Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… XPath
                    url_element = page.query_selector('xpath=//*[@id="react-root"]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/div[1]/div[2]/div[4]/div/a/span')

                    urls_in_user_url = []
                    if url_element:
                        element_text = url_element.inner_text().strip()
                        print(f"ğŸ”— Ø±Ø§Ø¨Ø· Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† UserUrl: {element_text}")

                        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ø§Ø¨Ø· ØµØ§Ù„Ø­ ÙÙŠ target_domains
                        if any(domain in element_text for domain in target_domains):
                            if not element_text.startswith("http"):
                                element_text = "https://" + element_text  # Ø¥Ø¶Ø§ÙØ© https:// Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
                            urls_in_user_url.append(element_text)

                    # Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø£ÙˆÙ„ 10 ØªØºØ±ÙŠØ¯Ø§Øª ÙˆØªØµÙÙŠØªÙ‡Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ `target_domains`
                    tweet_links = []
                    tweets = page.locator('div[data-testid="tweetText"]').all()[:10]
                    for tweet in tweets:
                        tweet_text = tweet.inner_text().strip()
                        tweet_urls = []
                        found_tweet_links = url_pattern.findall(tweet_text)

                        for match in found_tweet_links:
                            protocol = "https://" if not match[0] else match[0]
                            full_url = protocol + match[2]

                            if any(domain in full_url for domain in target_domains):
                                tweet_urls.append(full_url)

                        if tweet_urls:
                            tweet_links.append({"text": tweet_text, "urls": tweet_urls})

                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨ {username}: {e}")
                    bio_links_text = []
                    urls_in_user_url = []
                    tweet_links = []

                # âœ… **Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙÙŠ `target_domains`ØŒ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø³Ø§Ø¨**
                if not bio_links_text and not urls_in_user_url and not tweet_links:
                    continue

                # Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ø¨Ø§ÙŠÙˆØŒ UserUrlØŒ ÙˆØ§Ù„ØªØºØ±ÙŠØ¯Ø§Øª ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© ÙˆØ§Ø­Ø¯Ø©
                links_to_save = bio_links_text + urls_in_user_url
                for tweet in tweet_links:
                    links_to_save.extend(tweet["urls"])

                # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù CSV
                with open(output_file, mode="a", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)
                    writer.writerow([username, ", ".join(links_to_save)])

                # âœ… **Ø·Ø¨Ø§Ø¹Ø© ÙÙ‚Ø· Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· ÙÙŠ `target_domains`**
                print(f"ğŸ‘¤ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {username}")
                if bio_links_text:
                    print(f"ğŸ”— Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„Ø¨Ø§ÙŠÙˆ: {', '.join(bio_links_text)}")
                if urls_in_user_url:
                    print(f"ğŸŒ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙŠ UserUrl: {', '.join(urls_in_user_url)}")
                if tweet_links:
                    for tweet in tweet_links:
                        print(f"ğŸ“¢ ØªØºØ±ÙŠØ¯Ø©: {tweet['text']}")
                        print(f"ğŸ”— Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„ØªØºØ±ÙŠØ¯Ø©: {', '.join(tweet['urls'])}")
                        print("-" * 50)

    browser.close()

status_label.config(text="âœ… ØªÙ… Ø§Ù„ØªÙ†ÙÙŠØ° Ø¨Ù†Ø¬Ø§Ø­!")
root.update()
root.after(3000, root.destroy)
root.mainloop()
sys.exit(0)
